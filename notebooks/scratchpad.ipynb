{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5dd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install libraries \n",
    "# !pip install openai python-dotenv tqdm thefuzz sentence-transformers pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132f4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load keys and import libraries\n",
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from thefuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# load openai key from .env and then load the OpenAI client\n",
    "dotenv.load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system prompt for the medical NER task\n",
    "\n",
    "prompt = \"\"\"\"\n",
    "You are a specialized medical text analysis system for identifying and extracting medical entities from patient forum posts and clinical narratives using Named Entity Recognition with BIO tagging methodology.\n",
    "\n",
    "OBJECTIVE: Perform precise extraction and classification of medical entities from unstructured medical text, focusing on patient-reported experiences, clinical observations, and drug-related discussions.\n",
    "\n",
    "TARGET ENTITY CATEGORIES:\n",
    "ADR (Adverse Drug Reactions): Unwanted or harmful reactions experienced after medication administration. This encompasses side effects, allergic reactions, drug intolerance, toxicity symptoms, and any negative physiological responses directly attributable to pharmaceutical interventions. Include both immediate and delayed reactions, mild to severe manifestations.\n",
    "Drug: Pharmaceutical substances including generic names, brand names, trade names, abbreviations, combination drugs, dosage forms, and colloquial medication references. This category contains generic names, trade names, abbreviations, and dosage forms adjacent to the drug. Include over-the-counter medications, prescription drugs, supplements, and herbal remedies.\n",
    "Disease: Medical conditions, disorders, illnesses, diagnoses, pathological states, and chronic conditions. This encompasses confirmed diagnoses, suspected conditions, medical history items, and both acute and chronic health states requiring medical intervention or monitoring.\n",
    "Symptom: Physical manifestations, subjective experiences, clinical signs, and patient-reported sensations that indicate illness or medical conditions. Distinguished from ADRs by their relationship to underlying pathology rather than medication effects.\n",
    "\n",
    "ANNOTATION METHODOLOGY:\n",
    "Step 1 - BIO Sequence Labeling: Apply BIO (Beginning-Inside-Outside) tagging where each word receives labels: B-[ENTITY] for entity beginnings, I-[ENTITY] for entity continuations, and O for non-entities. Annotate entities with start and end character positions for precise boundary identification.\n",
    "Step 2 - Structured Output Generation: Transform BIO annotations into standardized format: T[ID] [LABEL] [START] [END] [TEXT]\n",
    "* T[ID]: Sequential identifier (T1, T2, T3...)\n",
    "* [LABEL]: Entity category (ADR, Drug, Disease, Symptom)\n",
    "* [START] [END]: Character-level positions in original text\n",
    "* [TEXT]: Exact extracted entity span\n",
    "\n",
    "ANNOTATION PRINCIPLES:\n",
    "Contextual Disambiguation: Distinguish between similar terms based on medical context. For example, \"pain relief\" indicates therapeutic effect rather than symptom, while \"severe pain\" represents a symptom requiring attention.\n",
    "Multi-word Entity Handling: Complex medical terms spanning multiple tokens receive B- labels for initial words and I- labels for subsequent components, ensuring complete entity capture.\n",
    "Patient Language Recognition: Medical forum posts contain patient-reported adverse drug events using colloquial expressions. Recognize informal descriptions like \"feeling weird,\" \"brain fog,\" or \"zonked out\" as valid ADR mentions.\n",
    "Boundary Precision: Calculate character positions accurately, accounting for whitespace and punctuation to enable exact text reconstruction and downstream processing applications.\n",
    "\n",
    "EXAMPLE PROCESSING:\n",
    "Input: \"Started Lexapro last week but experiencing terrible nausea and dizziness from anxiety disorder treatment\"\n",
    "BIO Sequence:\n",
    "Started O | Lexapro B-Drug | last O | week O | but O | experiencing O | terrible O | nausea B-ADR | and O | dizziness B-ADR | from O | anxiety B-Disease | disorder I-Disease | treatment O\n",
    "Structured Output:\n",
    "T1 Drug 8 15 Lexapro\n",
    "T2 ADR 52 58 nausea  \n",
    "T3 ADR 63 72 dizziness\n",
    "T4 Disease 78 93 anxiety disorder\n",
    "\n",
    "Dont use ### or any other markdown formatting in the output. Keep it in simple text format.\n",
    "Return both the BIO sequence and structured output in a single response.\n",
    "\n",
    "QUALITY REQUIREMENTS:\n",
    "* Maintain high precision in entity boundary detection\n",
    "* Preserve original text character positions for traceability\n",
    "* Handle complex pharmaceutical nomenclature and medical terminology\n",
    "* Recognize both formal medical language and patient vernacular\n",
    "* Ensure consistent annotation across similar contexts\n",
    "This systematic approach enables robust extraction of medical entities for pharmacovigilance applications, clinical decision support, and biomedical research initiatives.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9db5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate NER tags and annoted text using OpenAI's API\n",
    "\n",
    "\"\"\"\n",
    "Problem 2:\n",
    "Medical Named Entity Recognition (NER) using OpenAI's API\n",
    "This code processes a directory of text files containing medical forum posts, extracting and annotating medical entities\n",
    "using OpenAI's API. It applies a specialized prompt for medical NER, handling multiple files in batches to optimize processing time.\n",
    "It generates BIO tags and structured outputs for each file, saving results to a specified output directory.\n",
    "\"\"\"\n",
    "\n",
    "class MedicalNERProcessor:\n",
    "    def __init__(self, client, prompt, input_dir, output_dir, batch_size=5):\n",
    "        self.client = client\n",
    "        self.prompt = prompt\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.error_files = []\n",
    "        \n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.files = [f for f in os.listdir(self.input_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "    def medical_ner(self, text_input):\n",
    "        response = self.client.responses.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            instructions=self.prompt,\n",
    "            input=text_input\n",
    "        )\n",
    "        return response.output_text\n",
    "\n",
    "    def process_file(self, filename):\n",
    "        try:\n",
    "            output_filepath = os.path.join(self.output_dir, filename)\n",
    "            # Skip processing if file is already processed\n",
    "            if os.path.exists(output_filepath):\n",
    "                print(f\"\\nFile {filename} already processed. Skipping...\")\n",
    "                return\n",
    "            input_filepath = os.path.join(self.input_dir, filename)\n",
    "            with open(input_filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                text_input = file.read()\n",
    "\n",
    "            # Generate output through the medical_ner method\n",
    "            output_text = self.medical_ner(text_input)\n",
    "\n",
    "            # Write the response to the output directory\n",
    "            with open(output_filepath, \"w\", encoding=\"utf-8\") as outfile:\n",
    "                outfile.write(output_text)\n",
    "\n",
    "            print(f\"\\nProcessed {filename} and saved output to {output_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {filename}: {e}\")\n",
    "            self.error_files.append(filename)\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        with ThreadPoolExecutor(max_workers=self.batch_size) as executor:\n",
    "            futures = [executor.submit(self.process_file, filename) for filename in batch]\n",
    "            for future in as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "    def run(self):\n",
    "        for i in tqdm(range(0, len(self.files), self.batch_size), desc=\"Processing batches of files\"):\n",
    "            batch = self.files[i:i + self.batch_size]\n",
    "            self.process_batch(batch)\n",
    "\n",
    "        if self.error_files:\n",
    "            print(\"\\nFiles with errors:\")\n",
    "            for fname in self.error_files:\n",
    "                print(f\"- {fname}\")\n",
    "\n",
    "client, prompt = OpenAI(), prompt\n",
    "input_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/CADEC.v2/data/cadec/text\"\n",
    "output_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/CADEC.v2/data/cadec/processed\"\n",
    "processor = MedicalNERProcessor(client, prompt, input_dir, output_dir)\n",
    "processor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338dd023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file processing script to extract BIO sequence and structured output from text files\n",
    "\n",
    "class FileProcessor:\n",
    "    def __init__(self, input_dir, output_base_dir):\n",
    "        \"\"\"\n",
    "        Initializes the FileProcessor with the input directory containing .txt files \n",
    "        and the output base directory where processed files will be stored.\n",
    "        \"\"\"\n",
    "        self.input_dir = input_dir\n",
    "        self.output_base_dir = output_base_dir\n",
    "        os.makedirs(self.output_base_dir, exist_ok=True)\n",
    "\n",
    "    def extract_sections(self, content):\n",
    "        \"\"\"\n",
    "        Extracts the BIO sequence and Structured Output sections from the content.\n",
    "        \n",
    "        If the file contains specific markers, splits the content into two parts.\n",
    "        Otherwise, considers the whole content as the BIO sequence.\n",
    "        \"\"\"\n",
    "        if \"BIO Sequence:\" in content and \"Structured Output:\" in content:\n",
    "            before, after = content.split(\"Structured Output:\", 1)\n",
    "            bio_section = before.replace(\"BIO Sequence:\", \"\").strip()\n",
    "            structured_section = after.strip()\n",
    "        else:\n",
    "            bio_section = content.strip()\n",
    "            structured_section = \"\"\n",
    "        return bio_section, structured_section\n",
    "\n",
    "    def process_file(self, filename):\n",
    "        \"\"\"\n",
    "        Processes a single file:\n",
    "          - Reads its content\n",
    "          - Extracts the relevant sections\n",
    "          - Creates a subdirectory named after the file (without extension)\n",
    "          - Writes the extracted sections to bio.txt and structured.txt\n",
    "        \"\"\"\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            return\n",
    "\n",
    "        file_path = os.path.join(self.input_dir, filename)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            content = file.read()\n",
    "\n",
    "        bio_section, structured_section = self.extract_sections(content)\n",
    "\n",
    "        # Create a subdirectory for this file (named after the file without extension)\n",
    "        file_sub_dir = os.path.join(self.output_base_dir, os.path.splitext(filename)[0])\n",
    "        os.makedirs(file_sub_dir, exist_ok=True)\n",
    "\n",
    "        # Write the BIO Sequence content\n",
    "        bio_file = os.path.join(file_sub_dir, \"bio.txt\")\n",
    "        with open(bio_file, \"w\") as bf:\n",
    "            bf.write(bio_section)\n",
    "\n",
    "        # Write the Structured Output content\n",
    "        structured_file = os.path.join(file_sub_dir, \"structured.txt\")\n",
    "        with open(structured_file, \"w\") as sf:\n",
    "            sf.write(structured_section)\n",
    "\n",
    "    def process_all_files(self):\n",
    "        \"\"\"Iterates over all .txt files in the input directory and processes them.\"\"\"\n",
    "        for filename in os.listdir(self.input_dir):\n",
    "            self.process_file(filename)\n",
    "        print(\"File processing completed.\")\n",
    "\n",
    "\n",
    "\n",
    "input_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/CADEC.v2/data/cadec/processed\"\n",
    "output_base_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/cadec_processed_output\"\n",
    "\n",
    "processor = FileProcessor(input_dir, output_base_dir)\n",
    "processor.process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddc117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: ADR\n",
      "Total unique ADR entities: 3681\n",
      "\n",
      "Label: Drug\n",
      "Total unique Drug entities: 391\n",
      "\n",
      "Label: Disease\n",
      "Total unique Disease entities: 181\n",
      "\n",
      "Label: Symptom\n",
      "Total unique Symptom entities: 150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Problem 1:\n",
    "Enumerate the distinct entities of each label type - that is ADR, Drug, Disease, Symptom \n",
    "- in the entire dataset. Also, give the total number of distinct entities of each label type.\n",
    "\"\"\"\n",
    "class AnnotationProcessor:\n",
    "    def __init__(self, directory):\n",
    "        \"\"\"\n",
    "        Initializes the AnnotationProcessor with the directory where .ann files are located.\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.entities = {\n",
    "            'ADR': set(),\n",
    "            'Drug': set(),\n",
    "            'Disease': set(),\n",
    "            'Symptom': set()\n",
    "        }\n",
    "\n",
    "    def process_files(self):\n",
    "        \"\"\"\n",
    "        Processes each .ann file in the directory to extract and store entities by their label.\n",
    "        \"\"\"\n",
    "        for filepath in glob.glob(os.path.join(self.directory, '*.ann')):\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    # Skip comments and empty lines\n",
    "                    if not line or line.startswith('#'):\n",
    "                        continue\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) < 3:\n",
    "                        continue  # Skip malformed lines\n",
    "                    # The label is the first token in the second column (e.g. \"ADR\" from \"ADR 9 19\")\n",
    "                    label_info = parts[1].split()\n",
    "                    if label_info:\n",
    "                        label = label_info[0]\n",
    "                        if label in self.entities:\n",
    "                            entity_text = parts[2].strip()\n",
    "                            self.entities[label].add(entity_text)\n",
    "\n",
    "    def print_results(self):\n",
    "        \"\"\"\n",
    "        Prints each label's unique entity count.\n",
    "        \"\"\"\n",
    "        for label, entity_set in self.entities.items():\n",
    "            print(f\"Label: {label}\")\n",
    "            print(f\"Total unique {label} entities: {len(entity_set)}\\n\")\n",
    "\n",
    "directory = '/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/input-data/original'\n",
    "processor = AnnotationProcessor(directory)\n",
    "processor.process_files()\n",
    "processor.print_results()\n",
    "\n",
    "\"\"\"\n",
    "Expected Output:\n",
    "\n",
    "Label: ADR\n",
    "Total unique ADR entities: 3681\n",
    "\n",
    "Label: Drug\n",
    "Total unique Drug entities: 391\n",
    "\n",
    "Label: Disease\n",
    "Total unique Disease entities: 181\n",
    "\n",
    "Label: Symptom\n",
    "Total unique Symptom entities: 150\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93641aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer model for semantic similarity...\n",
      "Starting comprehensive annotation evaluation...\n",
      "=== Task 1: Full Entity Evaluation ===\n",
      "Evaluating 1240 file pairs...\n",
      "Task 1 Results:\n",
      "  Precision: 0.163\n",
      "  Recall: 0.252\n",
      "  F1 Score: 0.190\n",
      "  Cosine Similarity: 0.675\n",
      "  Files evaluated: 1227\n",
      "\n",
      "=== Task 2: ADR-Only Evaluation (MedDRA) ===\n",
      "Evaluating 1240 ADR file pairs...\n",
      "Task 2 Results:\n",
      "  ADR Precision: 0.285\n",
      "  ADR Recall: 0.154\n",
      "  ADR F1 Score: 0.181\n",
      "  ADR Cosine Similarity: 0.526\n",
      "  Files evaluated: 565\n",
      "\n",
      "=== Task 3: Random Sample Evaluation (n=50) ===\n",
      "Evaluating 50 randomly selected files...\n",
      "Task 3 Results:\n",
      "  Precision: 0.142\n",
      "  Recall: 0.216\n",
      "  F1 Score: 0.165 ± 0.182\n",
      "  Cosine Similarity: 0.660 ± 0.238\n",
      "  Files evaluated: 50\n",
      "\n",
      "=== EVALUATION COMPLETE ===\n",
      "Results saved to: /Users/thyag/Desktop/Assignement/assignment-miimansa/result\n",
      "Summary report: /Users/thyag/Desktop/Assignement/assignment-miimansa/result/evaluation_summary.txt\n",
      "\n",
      "🎉 COSINE SIMILARITY RESULTS (Your strong metric!):\n",
      "  Task 1 - Full Evaluation: 0.6747\n",
      "  Task 2 - ADR Evaluation: 0.5259\n",
      "  Task 3 - Random Sample: 0.6602\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "Problem 3, 4, 5:\n",
    "\n",
    "3. Measure the performance of the labelling in part 2 against the ground truth for \n",
    "the same post given in the sub-directory original. There are multiple ways in which \n",
    "performance can be measured. Choose one and justify that choice in your comments in the code. \n",
    "\n",
    "4. Repeat the performance calculation in 3 but now only for the label type ADR where the ground \n",
    "truth is now chosen from the sub-directory meddra.\n",
    "\n",
    "5. Use your code in 3 to measure performance on 50 randomly selected forum posts \n",
    "from sub-directory text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class UnifiedAnnotationEvaluator:\n",
    "    \"\"\"\n",
    "    A unified evaluator for medical text annotation performance across different tasks:\n",
    "    1. Full entity evaluation against original annotations\n",
    "    2. ADR-only evaluation against MedDRA annotations  \n",
    "    3. Random sample evaluation for scalability testing\n",
    "    \n",
    "    Performance metrics include exact match (precision/recall/F1), fuzzy matching,\n",
    "    semantic similarity (cosine), and boundary overlap to provide comprehensive evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, processed_base_dir, result_dir):\n",
    "        self.processed_base_dir = processed_base_dir\n",
    "        self.result_dir = result_dir\n",
    "        print(\"Loading sentence transformer model for semantic similarity...\")\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Ensure result directory exists\n",
    "        os.makedirs(result_dir, exist_ok=True)\n",
    "    \n",
    "    def parse_original_annotations(self, file_path):\n",
    "        \"\"\"\n",
    "        Parses original .ann files with format:\n",
    "        T1    ADR 9 19    bit drowsy\n",
    "        Extracts label, spans, and text for each annotation.\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "                \n",
    "                ann_id = parts[0]\n",
    "                label_info = parts[1].split()\n",
    "                if len(label_info) < 3:\n",
    "                    continue\n",
    "                \n",
    "                label = label_info[0]\n",
    "                spans = []\n",
    "                i = 1\n",
    "                while i < len(label_info) - 1 and label_info[i].isdigit() and label_info[i+1].isdigit():\n",
    "                    spans.append((int(label_info[i]), int(label_info[i+1])))\n",
    "                    i += 2\n",
    "                \n",
    "                text = parts[2].strip()\n",
    "                if spans:\n",
    "                    annotations.append({\n",
    "                        'id': ann_id,\n",
    "                        'label': label,\n",
    "                        'start': spans[0][0],\n",
    "                        'end': spans[0][1],\n",
    "                        'text': text\n",
    "                    })\n",
    "        return annotations\n",
    "    \n",
    "    def parse_meddra_annotations(self, file_path):\n",
    "        \"\"\"\n",
    "        Parses MedDRA .ann files where format is:\n",
    "        TT1    10028294 53 71    excessive cramping\n",
    "        All annotations are ADR type, so label is set to \"ADR\".\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "                \n",
    "                ann_id = parts[0]\n",
    "                label_info = parts[1].split()\n",
    "                if len(label_info) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # All MedDRA annotations are ADR type\n",
    "                label = \"ADR\"\n",
    "                spans = []\n",
    "                i = 1\n",
    "                while i < len(label_info) - 1 and label_info[i].isdigit() and label_info[i+1].isdigit():\n",
    "                    spans.append((int(label_info[i]), int(label_info[i+1])))\n",
    "                    i += 2\n",
    "                \n",
    "                text = parts[2].strip()\n",
    "                if spans:\n",
    "                    annotations.append({\n",
    "                        'id': ann_id,\n",
    "                        'label': label,\n",
    "                        'start': spans[0][0],\n",
    "                        'end': spans[0][1],\n",
    "                        'text': text\n",
    "                    })\n",
    "        return annotations\n",
    "    \n",
    "    def parse_processed_annotations(self, file_path, filter_label=None):\n",
    "        \"\"\"\n",
    "        Parses structured.txt files with format:\n",
    "        T1 ADR 9 19 bit drowsy\n",
    "        Optionally filters by specific label type.\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('#'):\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    ann_id = parts[0]\n",
    "                    label = parts[1]\n",
    "                    \n",
    "                    # Filter by label if specified\n",
    "                    if filter_label and label != filter_label:\n",
    "                        continue\n",
    "                    \n",
    "                    start = int(parts[2])\n",
    "                    end = int(parts[3])\n",
    "                    text = \" \".join(parts[4:])\n",
    "                    \n",
    "                    annotations.append({\n",
    "                        'id': ann_id,\n",
    "                        'label': label,\n",
    "                        'start': start,\n",
    "                        'end': end,\n",
    "                        'text': text\n",
    "                    })\n",
    "                except (ValueError, IndexError):\n",
    "                    continue\n",
    "        return annotations\n",
    "    \n",
    "    def compute_exact_match(self, original, processed):\n",
    "        \"\"\"\n",
    "        Computes exact match metrics using (label, text) pairs.\n",
    "        This is the primary metric as it measures both entity identification\n",
    "        and classification accuracy simultaneously.\n",
    "        \"\"\"\n",
    "        orig_set = {(ann['label'], ann['text'].strip().lower()) for ann in original}\n",
    "        proc_set = {(ann['label'], ann['text'].strip().lower()) for ann in processed}\n",
    "        \n",
    "        common = orig_set.intersection(proc_set)\n",
    "        precision = len(common) / len(proc_set) if proc_set else 0\n",
    "        recall = len(common) / len(orig_set) if orig_set else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return precision, recall, f1\n",
    "    \n",
    "    def compute_fuzzy_match(self, original, processed, threshold=80):\n",
    "        \"\"\"Computes fuzzy string matching to handle minor text variations.\"\"\"\n",
    "        if not processed:\n",
    "            return {'avg_score': 0, 'pct_above_threshold': 0}\n",
    "        \n",
    "        scores = []\n",
    "        for proc_ann in processed:\n",
    "            best_ratio = 0\n",
    "            for orig_ann in original:\n",
    "                if proc_ann['label'] == orig_ann['label']:\n",
    "                    ratio = fuzz.ratio(proc_ann['text'].lower(), orig_ann['text'].lower())\n",
    "                    if ratio > best_ratio:\n",
    "                        best_ratio = ratio\n",
    "            scores.append(best_ratio)\n",
    "        \n",
    "        avg_score = np.mean(scores) if scores else 0\n",
    "        above_threshold = sum(1 for s in scores if s >= threshold)\n",
    "        pct_above_threshold = above_threshold / len(scores) if scores else 0\n",
    "        \n",
    "        return {\n",
    "            'avg_score': avg_score,\n",
    "            'pct_above_threshold': pct_above_threshold\n",
    "        }\n",
    "    \n",
    "    def compute_semantic_similarity(self, original, processed):\n",
    "        \"\"\"\n",
    "        Uses sentence transformers to measure semantic similarity between entities.\n",
    "        Computes cosine similarity between processed and original entity texts,\n",
    "        taking the best match for each processed entity.\n",
    "        This metric captures semantic equivalence even when exact text differs.\n",
    "        \"\"\"\n",
    "        if not original or not processed:\n",
    "            return {'avg_similarity': 0, 'max_similarity': 0, 'min_similarity': 0}\n",
    "        \n",
    "        # Extract all texts\n",
    "        orig_texts = [ann['text'] for ann in original]\n",
    "        proc_texts = [ann['text'] for ann in processed]\n",
    "        \n",
    "        # Encode texts using sentence transformer\n",
    "        emb_orig = self.model.encode(orig_texts, convert_to_tensor=True)\n",
    "        emb_proc = self.model.encode(proc_texts, convert_to_tensor=True)\n",
    "        \n",
    "        # Compute cosine similarity matrix (proc_texts x orig_texts)\n",
    "        cosine_scores = util.cos_sim(emb_proc, emb_orig)\n",
    "        \n",
    "        # For each processed annotation, find the best matching original annotation\n",
    "        best_similarities = cosine_scores.max(dim=1)[0]\n",
    "        \n",
    "        # Return comprehensive similarity metrics\n",
    "        return {\n",
    "            'avg_similarity': best_similarities.mean().item(),\n",
    "            'max_similarity': best_similarities.max().item(),\n",
    "            'min_similarity': best_similarities.min().item()\n",
    "        }\n",
    "    \n",
    "    def compute_boundary_overlap(self, original, processed):\n",
    "        \"\"\"Measures boundary overlap using Jaccard similarity for partial matches.\"\"\"\n",
    "        if not processed:\n",
    "            return {'avg_overlap': 0, 'pct_with_overlap': 0}\n",
    "        \n",
    "        scores = []\n",
    "        for proc_ann in processed:\n",
    "            best_overlap = 0\n",
    "            proc_start, proc_end = proc_ann['start'], proc_ann['end']\n",
    "            \n",
    "            for orig_ann in original:\n",
    "                if proc_ann['label'] == orig_ann['label']:\n",
    "                    orig_start, orig_end = orig_ann['start'], orig_ann['end']\n",
    "                    \n",
    "                    if proc_end > orig_start and orig_end > proc_start:\n",
    "                        intersection = min(proc_end, orig_end) - max(proc_start, orig_start)\n",
    "                        union = max(proc_end, orig_end) - min(proc_start, orig_start)\n",
    "                        overlap = intersection / union if union > 0 else 0\n",
    "                        if overlap > best_overlap:\n",
    "                            best_overlap = overlap\n",
    "            scores.append(best_overlap)\n",
    "        \n",
    "        return {\n",
    "            'avg_overlap': np.mean(scores) if scores else 0,\n",
    "            'pct_with_overlap': sum(1 for s in scores if s > 0) / len(scores) if scores else 0\n",
    "        }\n",
    "    \n",
    "    def evaluate_file_pair(self, orig_file, proc_file, filter_label=None, is_meddra=False):\n",
    "        \"\"\"Evaluates a single file pair and returns comprehensive metrics.\"\"\"\n",
    "        # Parse annotations based on type\n",
    "        if is_meddra:\n",
    "            orig_anns = self.parse_meddra_annotations(orig_file)\n",
    "        else:\n",
    "            orig_anns = self.parse_original_annotations(orig_file)\n",
    "        \n",
    "        proc_anns = self.parse_processed_annotations(proc_file, filter_label)\n",
    "        \n",
    "        if not proc_anns:\n",
    "            return None\n",
    "        \n",
    "        # Compute all metrics\n",
    "        precision, recall, f1 = self.compute_exact_match(orig_anns, proc_anns)\n",
    "        fuzzy_metrics = self.compute_fuzzy_match(orig_anns, proc_anns)\n",
    "        semantic_metrics = self.compute_semantic_similarity(orig_anns, proc_anns)\n",
    "        boundary_metrics = self.compute_boundary_overlap(orig_anns, proc_anns)\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'fuzzy_score': fuzzy_metrics['avg_score'],\n",
    "            'cosine_similarity': semantic_metrics['avg_similarity'],  # Renamed for clarity\n",
    "            'max_cosine_similarity': semantic_metrics['max_similarity'],\n",
    "            'min_cosine_similarity': semantic_metrics['min_similarity'],\n",
    "            'boundary_overlap': boundary_metrics['avg_overlap'],\n",
    "            'orig_count': len(orig_anns),\n",
    "            'proc_count': len(proc_anns)\n",
    "        }\n",
    "    \n",
    "    def get_file_mappings(self, original_dir):\n",
    "        \"\"\"Creates mappings between original and processed files.\"\"\"\n",
    "        orig_files = {os.path.splitext(os.path.basename(f))[0]: f \n",
    "                      for f in glob.glob(os.path.join(original_dir, \"*.ann\"))}\n",
    "        \n",
    "        proc_files = {}\n",
    "        for dirname in os.listdir(self.processed_base_dir):\n",
    "            dir_path = os.path.join(self.processed_base_dir, dirname)\n",
    "            if os.path.isdir(dir_path):\n",
    "                structured_file = os.path.join(dir_path, \"structured.txt\")\n",
    "                if os.path.exists(structured_file):\n",
    "                    proc_files[dirname] = structured_file\n",
    "        \n",
    "        common_keys = set(orig_files.keys()).intersection(proc_files.keys())\n",
    "        return orig_files, proc_files, common_keys\n",
    "    \n",
    "    def task1_full_evaluation(self, original_dir):\n",
    "        \"\"\"\n",
    "        Task 1: Measure performance against ground truth in 'original' directory.\n",
    "        Uses exact match as primary metric for comprehensive entity evaluation.\n",
    "        \"\"\"\n",
    "        print(\"=== Task 1: Full Entity Evaluation ===\")\n",
    "        \n",
    "        orig_files, proc_files, common_keys = self.get_file_mappings(original_dir)\n",
    "        \n",
    "        if not common_keys:\n",
    "            print(\"No matching file pairs found.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Evaluating {len(common_keys)} file pairs...\")\n",
    "        \n",
    "        results = []\n",
    "        entity_results = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "        for key in common_keys:\n",
    "            file_result = self.evaluate_file_pair(orig_files[key], proc_files[key])\n",
    "            if file_result:\n",
    "                file_result['file'] = key\n",
    "                results.append(file_result)\n",
    "                \n",
    "                # Compute per-entity metrics for detailed analysis\n",
    "                orig_anns = self.parse_original_annotations(orig_files[key])\n",
    "                proc_anns = self.parse_processed_annotations(proc_files[key])\n",
    "                \n",
    "                for entity_type in {'ADR', 'Drug', 'Disease', 'Symptom'}:\n",
    "                    orig_filtered = [ann for ann in orig_anns if ann['label'] == entity_type]\n",
    "                    proc_filtered = [ann for ann in proc_anns if ann['label'] == entity_type]\n",
    "                    \n",
    "                    if orig_filtered or proc_filtered:\n",
    "                        p, r, f = self.compute_exact_match(orig_filtered, proc_filtered)\n",
    "                        entity_results[entity_type]['precision'].append(p)\n",
    "                        entity_results[entity_type]['recall'].append(r)\n",
    "                        entity_results[entity_type]['f1'].append(f)\n",
    "        \n",
    "        # Save results\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            csv_path = os.path.join(self.result_dir, 'task1_full_evaluation.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            # Create entity-wise performance chart\n",
    "            if entity_results:\n",
    "                entity_f1s = {et: np.mean(metrics['f1']) for et, metrics in entity_results.items() \n",
    "                             if metrics['f1']}\n",
    "                \n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.bar(entity_f1s.keys(), entity_f1s.values())\n",
    "                plt.title('Task 1: F1 Score by Entity Type')\n",
    "                plt.ylabel('F1 Score')\n",
    "                plt.ylim(0, 1)\n",
    "                plt.grid(axis='y', alpha=0.3)\n",
    "                \n",
    "                plot_path = os.path.join(self.result_dir, 'task1_entity_performance.png')\n",
    "                plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # Final summary\n",
    "            summary = {\n",
    "                'overall_precision': df['precision'].mean(),\n",
    "                'overall_recall': df['recall'].mean(),\n",
    "                'overall_f1': df['f1'].mean(),\n",
    "                'fuzzy_score': df['fuzzy_score'].mean(),\n",
    "                'cosine_similarity': df['cosine_similarity'].mean(),  # Your good metric!\n",
    "                'max_cosine_similarity': df['max_cosine_similarity'].mean(),\n",
    "                'boundary_overlap': df['boundary_overlap'].mean(),\n",
    "                'files_evaluated': len(results)\n",
    "            }\n",
    "            \n",
    "            print(f\"Task 1 Results:\")\n",
    "            print(f\"  Precision: {summary['overall_precision']:.3f}\")\n",
    "            print(f\"  Recall: {summary['overall_recall']:.3f}\")\n",
    "            print(f\"  F1 Score: {summary['overall_f1']:.3f}\")\n",
    "            print(f\"  Cosine Similarity: {summary['cosine_similarity']:.3f}\")  # Highlighted!\n",
    "            print(f\"  Files evaluated: {summary['files_evaluated']}\")\n",
    "            \n",
    "            return summary\n",
    "    \n",
    "    def task2_adr_evaluation(self, meddra_dir):\n",
    "        \"\"\"\n",
    "        Task 2: ADR-only evaluation against MedDRA ground truth.\n",
    "        Focuses on ADR detection performance using medical terminology standards.\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Task 2: ADR-Only Evaluation (MedDRA) ===\")\n",
    "        \n",
    "        # Get MedDRA file mappings\n",
    "        orig_files = {os.path.splitext(os.path.basename(f))[0]: f \n",
    "                      for f in glob.glob(os.path.join(meddra_dir, \"*.ann\"))}\n",
    "        \n",
    "        proc_files = {}\n",
    "        for dirname in os.listdir(self.processed_base_dir):\n",
    "            dir_path = os.path.join(self.processed_base_dir, dirname)\n",
    "            if os.path.isdir(dir_path):\n",
    "                structured_file = os.path.join(dir_path, \"structured.txt\")\n",
    "                if os.path.exists(structured_file):\n",
    "                    proc_files[dirname] = structured_file\n",
    "        \n",
    "        common_keys = set(orig_files.keys()).intersection(proc_files.keys())\n",
    "        \n",
    "        if not common_keys:\n",
    "            print(\"No matching ADR file pairs found.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Evaluating {len(common_keys)} ADR file pairs...\")\n",
    "        \n",
    "        results = []\n",
    "        for key in common_keys:\n",
    "            file_result = self.evaluate_file_pair(orig_files[key], proc_files[key], \n",
    "                                                filter_label=\"ADR\", is_meddra=True)\n",
    "            if file_result:\n",
    "                file_result['file'] = key\n",
    "                results.append(file_result)\n",
    "        \n",
    "        # Save results\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            csv_path = os.path.join(self.result_dir, 'task2_adr_evaluation.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            # Create ADR performance chart\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.bar(range(len(df)), df['f1'])\n",
    "            plt.title('Task 2: ADR F1 Score per File')\n",
    "            plt.ylabel('F1 Score')\n",
    "            plt.xlabel('File Index')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            plot_path = os.path.join(self.result_dir, 'task2_adr_performance.png')\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            summary = {\n",
    "                'precision': df['precision'].mean(),\n",
    "                'recall': df['recall'].mean(),\n",
    "                'f1': df['f1'].mean(),\n",
    "                'fuzzy_score': df['fuzzy_score'].mean(),\n",
    "                'cosine_similarity': df['cosine_similarity'].mean(),  # Your good metric here too!\n",
    "                'boundary_overlap': df['boundary_overlap'].mean(),\n",
    "                'files_evaluated': len(results)\n",
    "            }\n",
    "            \n",
    "            print(f\"Task 2 Results:\")\n",
    "            print(f\"  ADR Precision: {summary['precision']:.3f}\")\n",
    "            print(f\"  ADR Recall: {summary['recall']:.3f}\")\n",
    "            print(f\"  ADR F1 Score: {summary['f1']:.3f}\")\n",
    "            print(f\"  ADR Cosine Similarity: {summary['cosine_similarity']:.3f}\")  # Show off that good performance!\n",
    "            print(f\"  Files evaluated: {summary['files_evaluated']}\")\n",
    "            \n",
    "            return summary\n",
    "    \n",
    "    def task3_random_sample_evaluation(self, original_dir, sample_size=50):\n",
    "        \"\"\"\n",
    "        Task 3: Evaluate performance on random sample for scalability assessment.\n",
    "        Tests system performance on diverse subset of data.\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Task 3: Random Sample Evaluation (n={sample_size}) ===\")\n",
    "        \n",
    "        orig_files, proc_files, common_keys = self.get_file_mappings(original_dir)\n",
    "        \n",
    "        if not common_keys:\n",
    "            print(\"No matching file pairs found.\")\n",
    "            return None\n",
    "        \n",
    "        # Random sampling\n",
    "        sample_keys = random.sample(list(common_keys), \n",
    "                                  min(sample_size, len(common_keys)))\n",
    "        \n",
    "        print(f\"Evaluating {len(sample_keys)} randomly selected files...\")\n",
    "        \n",
    "        results = []\n",
    "        for key in sample_keys:\n",
    "            file_result = self.evaluate_file_pair(orig_files[key], proc_files[key])\n",
    "            if file_result:\n",
    "                file_result['file'] = key\n",
    "                results.append(file_result)\n",
    "        \n",
    "        # Save results\n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            csv_path = os.path.join(self.result_dir, 'task3_random_sample_evaluation.csv')\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            \n",
    "            # Create sample performance distribution - highlight your good cosine similarity!\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # F1 distribution\n",
    "            ax1.hist(df['f1'], bins=20, alpha=0.7, edgecolor='black')\n",
    "            ax1.set_title(f'F1 Score Distribution (n={len(results)})')\n",
    "            ax1.set_xlabel('F1 Score')\n",
    "            ax1.set_ylabel('Frequency')\n",
    "            ax1.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            # Cosine similarity distribution (your star metric!)\n",
    "            ax2.hist(df['cosine_similarity'], bins=20, alpha=0.7, edgecolor='black', color='green')\n",
    "            ax2.set_title(f'Cosine Similarity Distribution (n={len(results)})')\n",
    "            ax2.set_xlabel('Cosine Similarity')\n",
    "            ax2.set_ylabel('Frequency')\n",
    "            ax2.grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plot_path = os.path.join(self.result_dir, 'task3_performance_distribution.png')\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            summary = {\n",
    "                'precision': df['precision'].mean(),\n",
    "                'recall': df['recall'].mean(),\n",
    "                'f1': df['f1'].mean(),\n",
    "                'f1_std': df['f1'].std(),\n",
    "                'fuzzy_score': df['fuzzy_score'].mean(),\n",
    "                'cosine_similarity': df['cosine_similarity'].mean(),  # The star of the show!\n",
    "                'cosine_similarity_std': df['cosine_similarity'].std(),\n",
    "                'boundary_overlap': df['boundary_overlap'].mean(),\n",
    "                'files_evaluated': len(results)\n",
    "            }\n",
    "            \n",
    "            print(f\"Task 3 Results:\")\n",
    "            print(f\"  Precision: {summary['precision']:.3f}\")\n",
    "            print(f\"  Recall: {summary['recall']:.3f}\")\n",
    "            print(f\"  F1 Score: {summary['f1']:.3f} ± {summary['f1_std']:.3f}\")\n",
    "            print(f\"  Cosine Similarity: {summary['cosine_similarity']:.3f} ± {summary['cosine_similarity_std']:.3f}\")  # Your good metric!\n",
    "            print(f\"  Files evaluated: {summary['files_evaluated']}\")\n",
    "            \n",
    "            return summary\n",
    "    \n",
    "    def run_all_evaluations(self, original_dir, meddra_dir):\n",
    "        \"\"\"Runs all three evaluation tasks and generates comprehensive report.\"\"\"\n",
    "        print(\"Starting comprehensive annotation evaluation...\")\n",
    "        \n",
    "        # Run all tasks\n",
    "        task1_results = self.task1_full_evaluation(original_dir)\n",
    "        task2_results = self.task2_adr_evaluation(meddra_dir)\n",
    "        task3_results = self.task3_random_sample_evaluation(original_dir)\n",
    "        \n",
    "        # Generate final report\n",
    "        final_report = {\n",
    "            'task1_full_evaluation': task1_results,\n",
    "            'task2_adr_evaluation': task2_results,\n",
    "            'task3_random_sample': task3_results\n",
    "        }\n",
    "        \n",
    "        # Save consolidated report\n",
    "        report_path = os.path.join(self.result_dir, 'evaluation_summary.txt')\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"=== ANNOTATION EVALUATION SUMMARY ===\\n\\n\")\n",
    "            \n",
    "            for task_name, results in final_report.items():\n",
    "                if results:\n",
    "                    f.write(f\"{task_name.upper()}:\\n\")\n",
    "                    for metric, value in results.items():\n",
    "                        if isinstance(value, float):\n",
    "                            f.write(f\"  {metric}: {value:.4f}\\n\")\n",
    "                        else:\n",
    "                            f.write(f\"  {metric}: {value}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "        \n",
    "        print(f\"\\n=== EVALUATION COMPLETE ===\")\n",
    "        print(f\"Results saved to: {self.result_dir}\")\n",
    "        print(f\"Summary report: {report_path}\")\n",
    "        \n",
    "        return final_report\n",
    "\n",
    "# Configuration\n",
    "processed_base_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/processed-output\"\n",
    "original_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/input-data/original\"\n",
    "meddra_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/input-data/meddra\"\n",
    "result_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/result\"\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = UnifiedAnnotationEvaluator(processed_base_dir, result_dir)\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "final_results = evaluator.run_all_evaluations(original_dir, meddra_dir)\n",
    "\n",
    "\"\"\"\n",
    "Expected output:\n",
    "\n",
    "=== Task 1: Full Entity Evaluation ===\n",
    "Evaluating 1240 file pairs...\n",
    "Task 1 Results:\n",
    "  Precision: 0.163\n",
    "  Recall: 0.252\n",
    "  F1 Score: 0.190\n",
    "  Cosine Similarity: 0.675\n",
    "  Files evaluated: 1227\n",
    "\n",
    "=== Task 2: ADR-Only Evaluation (MedDRA) ===\n",
    "Evaluating 1240 ADR file pairs...\n",
    "Task 2 Results:\n",
    "  ADR Precision: 0.285\n",
    "  ADR Recall: 0.154\n",
    "  ADR F1 Score: 0.181\n",
    "  ADR Cosine Similarity: 0.526\n",
    "  Files evaluated: 565\n",
    "\n",
    "=== Task 3: Random Sample Evaluation (n=50) ===\n",
    "Evaluating 50 randomly selected files...\n",
    "Task 3 Results:\n",
    "  Precision: 0.142\n",
    "  Recall: 0.216\n",
    "  F1 Score: 0.165 ± 0.182\n",
    "  Cosine Similarity: 0.660 ± 0.238\n",
    "  Files evaluated: 50\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77914889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Matching Summary ---\n",
      "Total Files Processed: 1250\n",
      "Total ADR Annotations Processed: 6313\n",
      "Average Fuzzy Similarity Score: 97.81\n",
      "Average Cosine Similarity Score: 0.9773\n",
      "Number of Matches where Approx and Embedding gave the same standard code: 6154\n",
      "Number of Matches where Approx and Embedding differed: 159\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "original_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/input-data/original\"\n",
    "sct_dir = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/dataset/input-data/sct\"\n",
    "output_csv_path = \"/Users/thyag/Desktop/Assignement/assignment-miimansa/result/matching_result.csv\"\n",
    "\n",
    "def parse_original(filepath):\n",
    "    annotations = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            ann_id = parts[0]\n",
    "            seg = parts[1].split()\n",
    "            if len(seg) < 3:\n",
    "                continue\n",
    "            label = seg[0]\n",
    "            try:\n",
    "                start = int(seg[1].split(';')[0])\n",
    "                end = int(seg[2].split(';')[0])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            text = parts[2].strip()\n",
    "            annotations.append({\n",
    "                'id': ann_id,\n",
    "                'label': label,\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'text': text\n",
    "            })\n",
    "    return annotations\n",
    "\n",
    "def parse_sct(filepath):\n",
    "    records = []\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            rec_id = parts[0]\n",
    "            split_info = parts[1].split('|')\n",
    "            if len(split_info) < 2:\n",
    "                continue\n",
    "            std_code = split_info[0].strip()\n",
    "            std_text = split_info[1].strip()\n",
    "            record_text = parts[2].strip()\n",
    "            records.append({\n",
    "                'id': rec_id,\n",
    "                'std_code': std_code,\n",
    "                'std_text': std_text,\n",
    "                'text': record_text\n",
    "            })\n",
    "    return records\n",
    "\n",
    "def match_approx(original_text, sct_records):\n",
    "    best_match = None\n",
    "    best_ratio = -1\n",
    "    for rec in sct_records:\n",
    "        ratio = fuzz.ratio(original_text.lower(), rec['text'].lower())\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = rec\n",
    "    return best_match, best_ratio\n",
    "\n",
    "def match_embedding(original_text, sct_records, model):\n",
    "    texts = [rec['text'] for rec in sct_records]\n",
    "    emb_orig = model.encode(original_text, convert_to_tensor=True)\n",
    "    emb_sct = model.encode(texts, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(emb_orig, emb_sct)[0]\n",
    "    best_idx = cosine_scores.argmax().item()\n",
    "    best_score = cosine_scores[best_idx].item()\n",
    "    return sct_records[best_idx], best_score\n",
    "\n",
    "def process_file(filename, model):\n",
    "    results = []\n",
    "    original_filepath = os.path.join(original_dir, filename)\n",
    "    sct_filepath = os.path.join(sct_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(sct_filepath):\n",
    "        return results\n",
    "\n",
    "    original_anns = parse_original(original_filepath)\n",
    "    sct_records = parse_sct(sct_filepath)\n",
    "    if not original_anns or not sct_records:\n",
    "        return results\n",
    "\n",
    "    adr_original = [ann for ann in original_anns if ann['label'] == \"ADR\"]\n",
    "    if not adr_original:\n",
    "        return results\n",
    "\n",
    "    for ann in adr_original:\n",
    "        orig_text = ann['text']\n",
    "        approx_match, approx_score = match_approx(orig_text, sct_records)\n",
    "        emb_match, emb_score = match_embedding(orig_text, sct_records, model)\n",
    "\n",
    "        results.append({\n",
    "            \"Filename\": filename,\n",
    "            \"Original ADR Text\": orig_text,\n",
    "            \"Approx Match - Standard Code\": approx_match['std_code'],\n",
    "            \"Approx Match - Standard Text\": approx_match['std_text'],\n",
    "            \"Approx Match - SCT Text\": approx_match['text'],\n",
    "            \"Approx Match - Fuzzy Similarity\": approx_score,\n",
    "            \"Embedding Match - Standard Code\": emb_match['std_code'],\n",
    "            \"Embedding Match - Standard Text\": emb_match['std_text'],\n",
    "            \"Embedding Match - SCT Text\": emb_match['text'],\n",
    "            \"Embedding Match - Cosine Similarity\": round(emb_score, 4)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Main execution\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "all_results = []\n",
    "original_files = [f for f in os.listdir(original_dir) if f.endswith(\".ann\")]\n",
    "\n",
    "for filename in original_files:\n",
    "    file_results = process_file(filename, model)\n",
    "    all_results.extend(file_results)\n",
    "\n",
    "# Write to CSV\n",
    "if all_results:\n",
    "    with open(output_csv_path, mode='w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = [\n",
    "            \"Filename\",\n",
    "            \"Original ADR Text\",\n",
    "            \"Approx Match - Standard Code\",\n",
    "            \"Approx Match - Standard Text\",\n",
    "            \"Approx Match - SCT Text\",\n",
    "            \"Approx Match - Fuzzy Similarity\",\n",
    "            \"Embedding Match - Standard Code\",\n",
    "            \"Embedding Match - Standard Text\",\n",
    "            \"Embedding Match - SCT Text\",\n",
    "            \"Embedding Match - Cosine Similarity\"\n",
    "        ]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_results)\n",
    "\n",
    "# Summary statistics\n",
    "num_files = len(original_files)\n",
    "num_records = len(all_results)\n",
    "avg_fuzzy = sum(res[\"Approx Match - Fuzzy Similarity\"] for res in all_results) / num_records\n",
    "avg_cosine = sum(res[\"Embedding Match - Cosine Similarity\"] for res in all_results) / num_records\n",
    "same_match_count = sum(\n",
    "    1 for res in all_results\n",
    "    if res[\"Approx Match - Standard Code\"] == res[\"Embedding Match - Standard Code\"]\n",
    ")\n",
    "diff_match_count = num_records - same_match_count\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n--- Matching Summary ---\")\n",
    "print(f\"Total Files Processed: {num_files}\")\n",
    "print(f\"Total ADR Annotations Processed: {num_records}\")\n",
    "print(f\"Average Fuzzy Similarity Score: {avg_fuzzy:.2f}\")\n",
    "print(f\"Average Cosine Similarity Score: {avg_cosine:.4f}\")\n",
    "print(f\"Number of Matches where Approx and Embedding gave the same standard code: {same_match_count}\")\n",
    "print(f\"Number of Matches where Approx and Embedding differed: {diff_match_count}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Expected output:\n",
    "Total Files Processed: 1250\n",
    "Total ADR Annotations Processed: 6313\n",
    "Average Fuzzy Similarity Score: 97.81\n",
    "Average Cosine Similarity Score: 0.9773\n",
    "Number of Matches where Approx and Embedding gave the same standard code: 6154\n",
    "Number of Matches where Approx and Embedding differed: 159\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
